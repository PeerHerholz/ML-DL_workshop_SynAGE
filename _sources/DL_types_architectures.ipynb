{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model types & architectures\n",
    "\n",
    "[Peer Herholz (he/him)](https://peerherholz.github.io/)  \n",
    "Postdoctoral researcher - [NeuroDataScience lab](https://neurodatascience.github.io/) at [MNI](https://www.mcgill.ca/neuro/)/[McGill](https://www.mcgill.ca/), [UNIQUE](https://sites.google.com/view/unique-neuro-ai)  \n",
    "Member - [BIDS](https://bids-specification.readthedocs.io/en/stable/), [ReproNim](https://www.repronim.org/), [Brainhack](https://brainhack.org/), [Neuromod](https://www.cneuromod.ca/), [OHBM SEA-SIG](https://ohbm-environment.org/) \n",
    "\n",
    "<img align=\"left\" src=\"https://raw.githubusercontent.com/G0RELLA/gorella_mwn/master/lecture/static/Twitter%20social%20icons%20-%20circle%20-%20blue.png\" alt=\"logo\" title=\"Twitter\" width=\"32\" height=\"20\" /> <img align=\"left\" src=\"https://raw.githubusercontent.com/G0RELLA/gorella_mwn/master/lecture/static/GitHub-Mark-120px-plus.png\" alt=\"logo\" title=\"Github\" width=\"30\" height=\"20\" />   &nbsp;&nbsp;@peerherholz \n",
    "\n",
    "<img align=\"right\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ml-dl_workshop.png\" alt=\"logo\" title=\"Github\" width=\"400\" height=\"280\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A brief recap & first overview\n",
    "\n",
    "<img align=\"right\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/AI.png\" alt=\"logo\" title=\"Github\" width=\"320\" height=\"120\" />\n",
    "\n",
    "**Artificial intelligence (AI)** is [intelligence](https://en.wikipedia.org/wiki/Intelligence) demonstrated by [machines](https://en.wikipedia.org/wiki/Machine), as opposed to the natural intelligence [displayed by humans](https://en.wikipedia.org/wiki/Human_intelligence) or [animals](https://en.wikipedia.org/wiki/Animal_cognition). Leading AI textbooks define the field as the study of [\"intelligent agents\"](https://en.wikipedia.org/wiki/Intelligent_agent): any system that perceives its environment and takes actions that maximize its chance of achieving its goals. Some popular accounts use the term \"artificial intelligence\" to describe machines that mimic \"cognitive\" functions that humans associate with the [human mind](https://en.wikipedia.org/wiki/Human_mind), such as \"learning\" and \"problem solving\", however this definition is rejected by major AI researchers.\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Artificial_intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img align=\"right\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/AI_ML.png\" alt=\"logo\" title=\"Github\" width=\"320\" height=\"120\" />\n",
    "\n",
    "**Machine learning (ML)** is the study of computer [algorithms](https://en.wikipedia.org/wiki/Algorithm) that can improve automatically through experience and by the use of data. It is seen as a part of [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence). Machine learning algorithms build a model based on sample data, known as [\"training data\"](https://en.wikipedia.org/wiki/Training_data), in order to make predictions or decisions without being explicitly programmed to do so. A subset of machine learning is closely related to [computational statistics](https://en.wikipedia.org/wiki/Computational_statistics), which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of [mathematical optimization](https://en.wikipedia.org/wiki/Mathematical_optimization) delivers methods, theory and application domains to the field of machine learning. [Data mining](https://en.wikipedia.org/wiki/Data_mining) is a related field of study, focusing on [exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis) through [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning). Some implementations of machine learning use data and [neural networks](https://en.wikipedia.org/wiki/Neural_networks) in a way that mimics the working of a biological brain.\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Machine_learning](https://en.wikipedia.org/wiki/Machine_learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/AI_ML_DL.png\" alt=\"logo\" title=\"Github\" width=\"320\" height=\"120\" />\n",
    "\n",
    "**Deep learning** (also known as deep structured learning) is part of a broader family of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) methods based on [artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_networks) with [representation learning](https://en.wikipedia.org/wiki/Representation_learning). Learning can be [supervised](https://en.wikipedia.org/wiki/Supervised_learning), [semi-supervised](https://en.wikipedia.org/wiki/Semi-supervised_learning) or [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning). [Artificial neural networks (ANNs)](https://en.wikipedia.org/wiki/Artificial_neural_network) were inspired by information processing and distributed communication nodes in [biological systems](https://en.wikipedia.org/wiki/Biological_system). ANNs have various differences from biological [brains](https://en.wikipedia.org/wiki/Brain). Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analogue. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear [perceptron](https://en.wikipedia.org/wiki/Perceptron) cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. \n",
    "\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Deep_learning](https://en.wikipedia.org/wiki/Deep_learning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- very important: **deep learning is machine learning**\n",
    "    - DL is a specific subset of ML\n",
    "    - structured vs. unstructured input\n",
    "    - linearity\n",
    "    - model architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- you and \"the machine\"\n",
    "    - ML models can become better at a specific task, however they need some form of guidance\n",
    "    - DL models in contrast require less human intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Why the buzz? \n",
    "\n",
    "    - works amazing on structured input\n",
    "    - highly flexible → universal function approximator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What are the challenges?\n",
    "\n",
    "    - large number of parameters → data hungry \n",
    "    - large number of hyper-parameters → difficult to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When do I use it?\n",
    "\n",
    "    - if you have highly-structured input, eg. medical images. \n",
    "    - you have a lot of data and computational resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/core_aspects_examples.png\" alt=\"logo\" title=\"Github\" width=\"500\" height=\"280\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why go `deep learning` in `neuroscience`? (all highly discussed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- complexity of biological systems\n",
    "    - integrate knowledge of biological systems in computational systems\n",
    "      (excitation vs. inhibition, normalization, LIF)\n",
    "    - linear-nonlinear processing\n",
    "    - utilize computational systems as `model systems`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why go `deep learning` in `neuroscience`? (all highly discussed)\n",
    "\n",
    "- limitations of \"simple models\"\n",
    "    - fail to capture diversity of biological systems\n",
    "      (response heterogeneity, sensitivity vs. specificity, etc.)\n",
    "    - fail to perform as good as biological systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why go `deep learning` in `neuroscience`? (all highly discussed)\n",
    "\n",
    "- addressing the \"why question\"\n",
    "    - why do biological systems work in the way they do\n",
    "    - insights into objectives and constraints defined by evolutionary pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aim(s) of this section\n",
    "\n",
    "- learn about basics behind deep learning, specifically artificial neural networks\n",
    "- become aware of central building blocks and aspects of artificial neural networks\n",
    "- get to know different model types and architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Outline for this section\n",
    "\n",
    "1. Deep learning - basics & reasoning\n",
    "    - learning problems\n",
    "    - representations\n",
    "2. From biological to artificial neural networks\n",
    "    - neurons \n",
    "    - universal function approximation\n",
    "3. components of ANNs\n",
    "    - building parts\n",
    "    - learning\n",
    "4. ANN architectures\n",
    "    - Multilayer perceptrons\n",
    "    - Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep learning - basics & reasoning\n",
    "\n",
    "- as said before: `deep learning` is (a subset of) `machine learning` \n",
    "- it thus includes the core aspects we talked about in the [previous section]() and builds upon them:\n",
    "    - different learning problems and resulting models/architectures\n",
    "    - loss function & optimization\n",
    "    - training, evaluation, validation\n",
    "    - biases & problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- this furthermore transfers to the key components you as a user has to think about\n",
    "    - objective function (What is the goal?)\n",
    "    - learning rule (How should weights be updated to improve the objective function?)\n",
    "    - network architecture (What are the network parts and how are they connected?)\n",
    "    - initialisation (How are weights initially defined?)\n",
    "    - environment (What kind of data is provided for/during the learning?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Learning problems\n",
    "\n",
    "As in [machine learning]() in general, we have `supervised` & `unsupervised learning problems` again:\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/supervised_unsupervised.png\" alt=\"logo\" title=\"Github\" width=\"1200\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, within the world of `deep learning`, we have three more `learning problems`:\n",
    "\n",
    "- [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning)\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/RL.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- [semi-supervised learning](https://en.wikipedia.org/wiki/Semi-supervised_learning)\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/semisupervised.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- [self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning)\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/self-supervised.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- depending on the data and task, these `learning problems` can be employed within a diverse set of [artificial neural network](https://en.wikipedia.org/wiki/Artificial_neural_network) architectures (most commonly):\n",
    "    - [Multilayer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n",
    "    - [Convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n",
    "    - [Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But why employ [artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### The problem of variance & how representations can help\n",
    "\n",
    "Think about all the things you as an `biological agent` do on a typical day ... Everything (most things) you do appear very easy to you. Then why is so hard for `artificial agents` to achieve a comparable `behavior` and/or `performance`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One major problem is the `variance` of the input we encounter which subsequently makes it very hard to find appropriate `transformations` that can lead to/help to achieve `generalizable behavior`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How about an example? We'll keep it very simple and focus on `recognizing` a certain `category` of the natural world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You all waited for it and now it's finally happening: cute cats! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- let's assume we want to learn to recognize, label and predict \"cats\" based on a set of images that look like this\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/cat_prototype.png\" alt=\"logo\" title=\"Github\" width=\"200\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- utilizing the `models` and `approaches` we talked about so far, we would use `predetermined transformations` (`features`) of our data `X`:\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/cat_ml.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- this constitutes a form of [inductive bias](https://en.wikipedia.org/wiki/Inductive_bias), i.e. `assumptions` we include in the `learning problem` and thus back into the respective `models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- however, this is by far not the only way we could encounter a cat ... there are a lots of sources of variation of our data `X`, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- illumination\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/cat_illumination.png\" alt=\"logo\" title=\"Github\" width=\"400\" height=\"250\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- deformation\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/cat_deformation.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- occlusion\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/cat_occlusion.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- background clutter\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/cat_background.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- and intraclass variation\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/cat_variation.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- these variations (and many more) are usually not accounted for and our mapping from `X` to `Y` would fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- what we want to learn to prevent this are `invariant representations` that capture `latent variables` which are variables you (most likely) cannot directly observe, but that affect the variables you can observe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/cat_dl.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- the \"simple models\" we talked about so far work with `predetermined transformations` and thus perform `shallow learning`, more \"complex models\" perform `deep learning` in their `hidden layers` to learn `representations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img align=\"center\" src=\"https://media1.giphy.com/media/26ufdipQqU2lhNA4g/giphy.gif?cid=ecf05e47wv88pqvnas5utdrw2qap9xn9lmjvwv4kn3qenjr9&rid=giphy.gif&ct=g\" alt=\"logo\" title=\"Github\" width=\"300\" height=\"300\" />\n",
    "\n",
    "<sub><sup><sub><sup><sup>https://media1.giphy.com/media/26ufdipQqU2lhNA4g/giphy.gif?cid=ecf05e47wv88pqvnas5utdrw2qap9xn9lmjvwv4kn3qenjr9&rid=giphy.gif&ct=g\n",
    "</sup></sup></sub></sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From biological to artificial neural neurons and networks\n",
    "\n",
    "- decades ago researchers started to create artificial neurons to tackle tasks \"conventional algorithms\" couldn't handle\n",
    "- inspired by the learning and performance of biological neurons and networks\n",
    "- mimic defining aspects of biological neurons and networks \n",
    "- examples are: [integrate and fire neurons](https://en.wikipedia.org/wiki/Biological_neuron_model#Leaky_integrate-and-fire), [rectified linear rate neuron](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), [perceptrons](https://en.wikipedia.org/wiki/Perceptron), [multilayer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron), [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network), [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network), [autoencoders](https://en.wikipedia.org/wiki/Autoencoder), [generative adversarial networks](https://en.wikipedia.org/wiki/Generative_adversarial_network) \n",
    "\n",
    "<img align=\"center\" src=\"https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg\" alt=\"logo\" title=\"Github\" width=\"300\" height=\"300\" />\n",
    "\n",
    "<sub><sup><sub><sup><sup>https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg\n",
    "</sup></sup></sub></sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- using biological neurons and networks as the basis for artificial neurons and networks might therefore also help to learn `invariant representations` that capture `latent variables`\n",
    "- `deep learning` = `representation learning`\n",
    "- our minds (most likely) contains `(invariant) representations` about the world that allow us to interact with it\n",
    "    - `task optimization`\n",
    "    - `generalizability` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Back to biology...\n",
    "\n",
    "- `neurons` receive one or more inputs\n",
    "    - [excitatory postsynaptic potentials](https://en.wikipedia.org/wiki/Excitatory_postsynaptic_potential)\n",
    "    - [inhibitory postsynaptic potentials](https://en.wikipedia.org/wiki/Inhibitory_postsynaptic_potential)\n",
    "-  inputs are summed up to produce an output\n",
    "    - an activation\n",
    "- inputs are separably [weighted](https://en.wikipedia.org/wiki/Weighting) and sum passed through a [non-linear function](https://en.wikipedia.org/wiki/Non-linear_function)\n",
    "    - [activation](https://en.wikipedia.org/wiki/Activation_function) or [transfer function](https://en.wikipedia.org/wiki/Transfer_function)\n",
    "\n",
    "<img align=\"right\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Neuron3.svg/2560px-Neuron3.svg.png\" alt=\"logo\" title=\"Github\" width=\"300\" height=\"300\" />\n",
    "\n",
    "<sub><sup><sub><sup><sup>https://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Neuron3.svg/2560px-Neuron3.svg.png\n",
    "</sup></sup></sub></sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- these processes can be translated into mathematical problems including the input `X`, its weights `W` and the activation function `f`\n",
    "\n",
    "<img align=\"center\" src=\"https://miro.medium.com/max/1400/1*BMSfafFNEpqGFCNU4smPkg.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"300\" />\n",
    "\n",
    "<sub><sup><sub><sup><sup>https://miro.medium.com/max/1400/1*BMSfafFNEpqGFCNU4smPkg.png\n",
    "</sup></sup></sub></sup></sub>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- the thing about `activation function`s...\n",
    "\n",
    "    - they define the resulting type of an `artificial neuron`\n",
    "    - thus they also define its capabilities\n",
    "    - require non-linearity\n",
    "        - because otherwise only linear functions and decision probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the thing about `activation function`s...\n",
    "\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\text { Non-linear transfer functions}\\\\\n",
    "\\begin{array}{llc}\n",
    "\\hline \\text { Name } & \\text { Formula } & \\text { Year } \\\\\n",
    "\\hline \\text { none } & \\mathrm{y}=\\mathrm{x} & - \\\\\n",
    "\\text { sigmoid } & \\mathrm{y}=\\frac{1}{1+e^{-x}} & 1986 \\\\\n",
    "\\tanh & \\mathrm{y}=\\frac{e^{2 x}-1}{e^{2 x}+1} & 1986 \\\\\n",
    "\\text { ReLU } & \\mathrm{y}=\\max (\\mathrm{x}, 0) & 2010 \\\\\n",
    "\\text { (centered) SoftPlus } & \\mathrm{y}=\\ln \\left(e^{x}+1\\right)-\\ln 2 & 2011 \\\\\n",
    "\\text { LReLU } & \\mathrm{y}=\\max (\\mathrm{x}, \\alpha \\mathrm{x}), \\alpha \\approx 0.01 & 2011 \\\\\n",
    "\\text { maxout } & \\mathrm{y}=\\max \\left(W_{1} \\mathrm{x}+b_{1}, W_{2} \\mathrm{x}+b_{2}\\right) & 2013 \\\\\n",
    "\\text { APL } & \\mathrm{y}=\\max (\\mathrm{x}, 0)+\\sum_{s=1}^{S} a_{i}^{s} \\max \\left(0,-x+b_{i}^{s}\\right) & 2014 \\\\\n",
    "\\text { VLReLU } & \\mathrm{y}=\\max (\\mathrm{x}, \\alpha \\mathrm{x}), \\alpha \\in 0.1,0.5 & 2014 \\\\\n",
    "\\text { RReLU } & \\mathrm{y}=\\max (\\mathrm{x}, \\alpha \\mathrm{x}), \\alpha=\\operatorname{random}(0.1,0.5) & 2015 \\\\\n",
    "\\text { PReLU } & \\mathrm{y}=\\max (\\mathrm{x}, \\alpha \\mathrm{x}), \\alpha \\text { is learnable } & 2015 \\\\\n",
    "\\text { ELU } & \\mathrm{y}=\\mathrm{x}, \\text { if } \\mathrm{x} \\geq 0, \\text { else } \\alpha\\left(e^{x}-1\\right) & 2015 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"400\"\n",
       "            src=\"https://polarisation.github.io/tfjs-activation-functions/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb2258d50b8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='https://polarisation.github.io/tfjs-activation-functions/', width=700, height=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- historically either [sigmoid](https://en.wikipedia.org/wiki/Logistic_function) or [tanh](https://en.wikipedia.org/wiki/Hyperbolic_function#Hyperbolic_tangent) utilized\n",
    "- even though they are [non-linear functions] their properties make them insufficient for most problems, especially `sigmoid`\n",
    "    - rather simple `polynomials`  \n",
    "    - mainly work for `binary problems`\n",
    "    - computationally expensive\n",
    "    - they saturate causing the neuron and thus network to \"die\", i.e. stop `learning`\n",
    "- modern `ANN` frequently use `continuous activation functions` like [Rectified Linear Unit](https://deepai.org/machine-learning-glossary-and-terms/rectified-linear-units)\n",
    "    - doesn't saturate\n",
    "    - faster training and convergence\n",
    "    - introduce network sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Still, the question is: how does this help us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's imagine the following situation:\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/UAT_problem.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "- we could try to iterate over all possible `transformations`/`functions` necessary to enable and/or optimize the `output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, we could also introduce a [hidden layer]() that learns or more precisely `approximates` what those `transformations`/`functions` are on its own:\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/UAT_hiddenlayer.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The idea: there is a `neural network` so that for every possible input `X`, the outcome is `f(X)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Importantly, the [hidden layer]() consists of [artificial neurons]() that perceive `weighted inputs` `w` and perform [non-linear]() ([non-saturating]()) [activation functions]() `v` which `output` will be used for the `task` at hand\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/UAT_hiddenlayer_function.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It gets even better: this holds true even if there are multiple `inputs` and `outputs`:\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/UAT_generalizability.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- this is referred to as `universality` and finally brings us to one core aspect of `deep learning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Universal function approximation theorem\n",
    "\n",
    "- `artificial neural networks` are considered `universal function approximators`\n",
    "    - the possibility of `approximating` a(ny) `function` to some accuracy with  \n",
    "      (a set of) [artificial neurons]() in [hidden layer](s)\n",
    "    - instead of providing a predetermined set of `transformations` or `functions`,\n",
    "      the `ANN` learns/approximates them by itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  two problems:\n",
    "    - the theorem doesn't tell us how many [artificial neurons we need]()\n",
    "    - either arbitrary number of artificial neurons (\"arbitrary width\" case) or\n",
    "      arbitrary number of hidden layers, each containing a limited number of artificial neurons (\"arbitrary depth\" \n",
    "      case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- going back to \"shallow learning\": we provide pre-extracted/pre-computed `features` of our `data` `X` and maybe apply further `preprocessing` before letting our model `M` `learns` the mapping to our outcome `Y` via `optimization` (minimizing the `loss function`) \n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/core_aspects_preprocessing.png\" alt=\"logo\" title=\"Github\" width=\"500\" height=\"280\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what `deep learning` does instead is to `learn` `features` by itself, namely those that are most useful for the `objective function`, i.e. `task` as defined by `optimization`\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/dl_features.png\" alt=\"logo\" title=\"Github\" width=\"500\" height=\"280\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To bring the things we talked about so far together, we will focus on `ANN` components and how `learning` takes place next...but at first, let's take a breather.\n",
    "\n",
    "<img align=\"center\" src=\"https://media4.giphy.com/media/1LmBFphV4XNSw/giphy.gif?cid=ecf05e47og07li3vrdt89rgz8uux1qjicb3ykg2z5qdgigu7&rid=giphy.gif&ct=g\" alt=\"logo\" title=\"Github\" width=\"300\" height=\"300\" />\n",
    "\n",
    "<sub><sup><sub><sup><sup>https://media4.giphy.com/media/1LmBFphV4XNSw/giphy.gif?cid=ecf05e47og07li3vrdt89rgz8uux1qjicb3ykg2z5qdgigu7&rid=giphy.gif&ct=g\n",
    "</sup></sup></sub></sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Components of `ANN`s\n",
    "\n",
    "- now that we've spent quite some time on the `neurobiological informed` underpinnings it's time to put the respective pieces together and see how they are actually employed within `ANN`s  \n",
    "- for this we will talk about two aspects:\n",
    "    - building blocks of `ANN`s\n",
    "    - learning in `ANN`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Building blocks of `ANN`s\n",
    "\n",
    "- we've actually already seen quite a few important building blocks before but didn't defined them appropriately\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/UAT_generalizability.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_layer.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "\n",
    "| Term         | Definition | \n",
    "|--------------|:-----:|\n",
    "| Layer |  Structure or network topology in the architecture of the model that consists of `nodes` and is connected to other layers, receiving and passing information. |\n",
    "| Input layer |  The layer that receives the external input data. |\n",
    "| Hidden layer(s) |  The layer(s) between `input` and `output layer` which performs `transformations` via `non-linear activation functions` . |\n",
    "| Output layer |  The layer that produces the final output/task. |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_subparts.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "\n",
    "| Term         | Definition | \n",
    "|--------------|:-----:|\n",
    "| Node |  `Artificial neurons`. |\n",
    "| Connection | Connection between `nodes`, providing `output` of one `node`/`neuron` as `input` to the next `node`/`neuron`.  |\n",
    "| Weight |  The relative importance of the `connection`. |\n",
    "| Bias |  The bias term that can be added to the `propagation function`, i.e. input to a neuron computed from the outputs of its predecessor neurons and their connections as a weighted sum. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ANN`s can be described based on their amount of `hidden layers` (`depth`, `width`)\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_multilayer.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- having talked about `overt building blocks` of `ANN`s we need to talk about `building blocks` that are rather `covert`, that is the aspects that define how `ANN`s learn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Learning in `ANN`s\n",
    "\n",
    "- let's go back a few hours and talk about `model fitting` again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- when talking about `model fitting`, we need to talk about three central aspects:\n",
    "    - the model\n",
    "    - the loss function\n",
    "    - the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| Term         | Definition | \n",
    "|--------------|:-----:|\n",
    "| Model |  A set of parameters that makes a prediction based on a given input. The parameter values are fitted to available data.|\n",
    "| Loss function | A function that evaluates how well your algorithm models your dataset |\n",
    "| Optimization | A function that tries to minimize the loss via updating model parameters. |\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### An example: linear regression\n",
    "\n",
    "- Model:  $$y=\\beta_{0}+\\beta_{1} x_{1}^{2}+\\beta_{2} x_{2}^{2}$$\n",
    "- Loss function: $$ M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$\n",
    "- optimization: [Gradient descent]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `Gradient descent` with a `single input variable` and `n samples`\n",
    "    - Start with random weights (`β0` and `β1`) $$\\hat{y}_{i}=\\beta_{0}+\\beta_{1} X_{i}$$\n",
    "    - Compute loss (i.e. `MSE`) $$M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$\n",
    "    - Update `weights` based on the `gradient`\n",
    "    \n",
    "<img align=\"center\" src=\"https://cdn.hackernoon.com/hn-images/0*D7zG46WrdKx54pbU.gif\" alt=\"logo\" title=\"Github\" width=\"550\" height=\"280\" />\n",
    "<sub><sup><sub><sup><sup>https://cdn.hackernoon.com/hn-images/0*D7zG46WrdKx54pbU.gif\n",
    "</sup></sup></sub></sup></sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `Gradient descent` for complex models with `non-convex loss functions`\n",
    "    - Start with random weights (`β0` and `β1`) $$\\hat{y}_{i}=\\beta_{0}+\\beta_{1} X_{i}$$\n",
    "    - Compute loss (i.e. `MSE`) $$M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$\n",
    "    - Update `weights` based on the `gradient`\n",
    "    \n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/gradient_descent_complex_models.png\" alt=\"logo\" title=\"Github\" width=\"500\" height=\"280\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ANN architectures\n",
    "\n",
    "- now that we've gone through the underlying basics and important building blocks of `ANN`s, we will check out a few of the most commonly used architectures\n",
    "- in general we can [group `ANN`s based on their `architecture`](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks), that is how their building blocks are defined and integrated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- possible `architectures` include (only a very tiny subset listed):\n",
    "    - [feedforward](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#Feedforward) (information moves in a forward fashion through the ANN, without cycles and/or loops)\n",
    "        - [Multilayer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n",
    "        - [Convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n",
    "        - [autoencoders](https://en.wikipedia.org/wiki/Autoencoder)\n",
    "    - [recurrent](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#Recurrent_neural_network) (information moves in a forward and a backward fashion through the ANN)\n",
    "        - [fully recurrent](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#Fully_recurrent)\n",
    "        - [Long short-term memory](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#Long_short-term_memory)\n",
    "    - [radial basis function](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#Radial_basis_function_(RBF)) (networks that use radial basis functions as activation function)\n",
    "        - [General regression network](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#General_regression_neural_network)\n",
    "        - [Deep belief networks](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#Deep_belief_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- we will spend a closer look at `feedforward` and `recurrent architectures` as they will (most likely) be the ones you see frequently utilized within `neuroscience` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- show sigmoid, tanh, ReLU and name problems of first two\n",
    "- not sufficient for universal function approximation, ReLU needed\n",
    "- explain universal function approximation and why at least one hidden layer necessary (learn functions themselves)\n",
    "- MLPs\n",
    "- CNNs\n",
    "- mention RNN, VAE, GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning problems, architectures \n",
    "- introduce common problems\n",
    "    - variation, etc.\n",
    "- importance representations to address those variations, learn latent variables\n",
    "    - how derive them?\n",
    "    - based on performance of biological systems folks start to create artificial neurons\n",
    "    - limitation of LIF (activation function) -> only linear\n",
    "- universal function approximators    \n",
    "    - missing non-linearity\n",
    "    - cumbersome to impossible to iterate over all possible functions and underlying parameters\n",
    "    - thus universal function approximators -> hidden layers learn functions by themselves\n",
    "- MLPs as most simple ANNs\n",
    "    - show example\n",
    "    - non-linear activation functions\n",
    "    - fully connected\n",
    "    - softmax layer\n",
    "    - outline problems with that which lead to CNNs\n",
    "- introduce CNNs with convolution, pooling layers, etc.    \n",
    "    - hierarchy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are fully connected layers required?\n",
    "We can divide the whole neural network (for classification) into two parts:\n",
    "\n",
    "Feature extraction: In the conventional classification algorithms, like SVMs, we used to extract features from the data to make the classification work. The convolutional layers are serving the same purpose of feature extraction. CNNs capture better representation of data and hence we don’t need to do feature engineering.\n",
    "Classification: After feature extraction we need to classify the data into various classes, this can be done using a fully connected (FC) neural network. In place of fully connected layers, we can also use a conventional classifier like SVM. But we generally end up adding FC layers to make the model end-to-end trainable. The fully connected layers learn a (possibly non-linear) function between the high-level features given as an output from the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- introduce indicative bias/hierarchy when introducing CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
